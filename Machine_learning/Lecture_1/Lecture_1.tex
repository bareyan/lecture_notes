```latex
\documentclass{article}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{listings}
\usepackage{caption}
\usepackage{float}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{hyperref}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{solution}{Solution}
\newtheorem{example}{Example}

\begin{document}
\sloppy

\section*{Chapter Title: The Learning Problem}

\section{Introduction}
\subsection{What is Machine Learning?}
Machine learning is a broad subject that spans from abstract theory to practical rules of thumb. It is useful because it provides a conceptual framework and practical tools to deal with real learning systems. The inclusion of a topic in a machine learning course depends on its relevance to the field itself, balancing mathematical foundations with practical applications.

\subsection{Course Storyline}
This chapter, and the course, will follow a story line to understand the fundamentals of learning, starting with the basic questions and progressing to more advanced concepts. The storyline is as follows:
\begin{itemize}
    \item What is learning?
    \item Can we learn?
    \item How to do it?
    \item How to do it well?
    \item Take-home lessons.
\end{itemize}
While the topics are presented in a logical sequence, some practical tools will be introduced early on to allow for experimentation with theoretical concepts, even before the complete theoretical framework is fully developed. The course will begin by building conceptual and theoretical foundations, and then transition towards more practical aspects of machine learning.

\section{The Essence of Machine Learning: A Movie Rating Example}
\subsection{Predicting Movie Ratings}
Consider the problem of predicting how a viewer would rate a movie. This is a pertinent problem for movie rental companies like Netflix, who famously offered a \$1 million prize for a mere 10\% improvement in their recommendation system. This improvement is valuable because better recommendations lead to increased user engagement and movie rentals, significantly boosting revenue. This example encapsulates the core of machine learning and its high-stakes applications in various fields, such as financial forecasting where even minute improvements can yield substantial financial gains.

\subsection{Three Components of Machine Learning}
The problem of movie rating effectively illustrates the three essential components that define a machine learning problem:
\begin{enumerate}
    \item \textbf{A pattern exists}: There is an underlying pattern in how viewers rate movies. A person's rating is related to their past ratings and how others have rated the same movie.
    \item \textbf{Mathematically intractable pattern}: This pattern, while real, cannot be easily described or formulated mathematically, for instance, through a simple polynomial equation.
    \item \textbf{Data availability}:  There must be data from which to learn this pattern.  Without data, there is nothing to learn from.
\end{enumerate}
The reliance on data to uncover patterns that are too complex to define mathematically is what necessitates and empowers machine learning.  Without these three components, especially the presence of data, applying machine learning techniques becomes impractical.

\subsection{A Proposed Solution}
One approach to movie rating prediction involves describing both viewers and movies as vectors of factors. These factors could represent various aspects such as comedy content, action content, preference for blockbusters, or even preferences regarding lead actors.

For a \textbf{viewer}, the vector would represent their taste profile:
\begin{itemize}
    \item Does the viewer like comedies?
    \item Does the viewer like action movies?
    \item Does the viewer prefer blockbusters or indie films?
    \item Does the viewer like specific actors?
    \item ... (and so on for hundreds of potential factors)
\end{itemize}

Similarly, for a \textbf{movie}, the vector would represent its content profile:
\begin{itemize}
    \item Does the movie have comedy content?
    \item Does the movie have action?
    \item Is it a blockbuster-style movie?
    \item Does it star specific actors?
    \item ... (corresponding factors to the viewer profile)
\end{itemize}

The prediction is made by matching the movie and viewer factors. If there is a high degree of match between a viewer's preferences and a movie's content across these factors, the predicted rating would be high. Conversely, mismatches would lead to lower predicted ratings.  The process involves:
\begin{enumerate}
    \item Matching movie and viewer factors.
    \item Adding the contributions from each factor match or mismatch.
    \item Outputting a predicted rating based on the aggregated contributions.
\end{enumerate}
However, this approach requires manual analysis of movies and viewers to determine their factor profiles, making it less of a 'learning' approach and more of an analytical one.

\subsection{A Learning Approach: Reverse Engineering from Ratings}
A true machine learning approach to movie ratings involves reverse-engineering the factor profiles directly from user ratings. Instead of pre-defining and analyzing movie and viewer factors, the learning process starts with observed ratings and works backward to infer these factors.

Initially, we can assume random factors for both viewers and movies. For each viewer and movie, assign random numerical values across all factors.  When a viewer rates a movie, the system compares the predicted rating (derived from the inner product of their random factor vectors) with the actual rating.  If there is a discrepancy, the system slightly adjusts the factors to move the predicted rating closer to the actual rating.

This adjustment process is repeated over a large dataset of ratingsâ€”potentially millions. By iteratively refining these random factors based on actual ratings, the system gradually learns meaningful and consistent factor vectors for viewers and movies.

After this learning process, when a new viewer who has not rated a particular movie is considered, the system can use the learned factor vectors. By calculating the inner product of the viewer's and the movie's factor vectors, the system can predict a rating that is likely to be consistent with how the viewer would actually rate the movie. This method, which emerged as a winning solution in the Netflix Prize competition, exemplifies how machine learning can automatically discover hidden patterns and relationships from data without explicit programming of rules or factor definitions.

\section{Formalizing the Learning Problem: Credit Approval Metaphor}
\subsection{Credit Approval Scenario}
To formalize the concept of learning, let's consider a different application: credit approval. Banks need to decide whether to grant credit to an applicant based on their information. Similar to movie ratings, there's no explicit formula to determine creditworthiness. Banks rely on historical data to make these decisions.

\subsection{Components and Notations}
In a credit approval scenario, we can identify key components of a learning problem and formalize them using mathematical notations.

\begin{enumerate}
    \item \textbf{Input} (\textbf{x}): This is the information provided by the applicant, such as age, income, debt, etc. Mathematically, this is represented as a d-dimensional vector $x = (x_1, x_2, ..., x_d)$, where each dimension corresponds to an attribute. The set of all possible input vectors is denoted by $X$.
    \item \textbf{Output} (\textbf{y}): This is the decision made by the bank, which is binary: approve credit (+1) or deny credit (-1). The set of possible outputs is $Y = \{+1, -1\}$.
    \item \textbf{Target Function} (\textbf{f}): This is the ideal credit approval formula, which is unknown. It's a function $f: X \rightarrow Y$ that perfectly determines if an applicant is creditworthy. In reality, this function is what we want to learn or approximate.
    \item \textbf{Data} (\textbf{D}): Historical records of previous customers. Each record consists of an applicant's information (input $x$) and their credit behavior in hindsight (output $y$). The dataset $D$ consists of $N$ such examples: $D = \{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\}$.
    \item \textbf{Hypothesis} (\textbf{g}): This is our approximation of the target function $f$.  After learning from the data $D$, we aim to find a hypothesis $g: X \rightarrow Y$ that is as close to $f$ as possible. Ideally, $g(x) \approx f(x)$ for most inputs $x$.
\end{enumerate}

\subsection{The Learning Diagram}
We can visualize the learning process using a diagram:

\begin{verbatim}
Target Function (f)  -->  Training Examples (D)
   (Unknown, Ideal)     (Vehicle to learn f)
        |                     |
        |  Learning           |  Final Hypothesis (g)
        |  Algorithm         |  (Approximation of f)
        --------------------->
              Hypothesis Set (H)
              (Set of candidate formulas)
\end{verbatim}


\begin{figure}[H]
    \centering
    \begin{verbatim}
#save_to: learning_diagram.png
from graphviz import Digraph

dot = Digraph('LearningDiagram', comment='Learning Process', graph_attr={'rankdir': 'TB'})

dot.node('TF', 'Target Function (f)\n(Unknown, Ideal)')
dot.node('TD', 'Training Examples (D)\n(Vehicle to learn f)')
dot.node('LA', 'Learning Algorithm')
dot.node('HS', 'Hypothesis Set (H)\n(Set of candidate formulas)')
dot.node('FH', 'Final Hypothesis (g)\n(Approximation of f)')

dot.edge('TF', 'TD')
dot.edge('TD', 'LA')
dot.edge('HS', 'LA')
dot.edge('LA', 'FH')
dot.edge('TD', 'FH', style='dashed', constraint='false') # Dashed edge to indicate direct relation conceptually

dot.render('learning_diagram', format='png', view=False)
    \end{verbatim}
    \includegraphics[max width=\textwidth, max height=0.4\textheight, keepaspectratio]{learning_diagram.png}
    \caption{Learning Diagram}
    \label{fig:learning_diagram}
\end{figure}


The target function $f$ is the actual relationship we want to learn, but it remains unknown. We only observe it through training examples $D$.  A hypothesis set $H$ is a collection of possible functions that our learning algorithm can choose from. The learning algorithm uses the training data $D$ to select a final hypothesis $g$ from $H$ that best approximates the target function $f$.  The hypothesis set is a crucial component as it defines the type of functions the learning algorithm will consider. Choosing an appropriate hypothesis set is important for effective learning.

\section{The Perceptron Model: A Simple Learning Algorithm}
\subsection{The Perceptron Hypothesis Set}
The perceptron model is a simple yet fundamental model in machine learning, particularly for binary classification problems like credit approval.

Given a d-dimensional input vector $x = (x_1, x_2, ..., x_d)$, the perceptron model uses a linear formula to make a decision.  It calculates a weighted sum of the input features and compares it to a threshold. The formula is:

\[
h(x) = \text{sign}\left( \left( \sum_{i=1}^{d} w_i x_i \right) - \text{threshold} \right)
\]
where:
\begin{itemize}
    \item $x = (x_1, x_2, ..., x_d)$ is the input vector (customer application).
    \item $w = (w_1, w_2, ..., w_d)$ is the weight vector, where each $w_i$ represents the importance of the $i$-th feature.
    \item $\text{threshold}$ is a value against which the weighted sum is compared.
    \item $\text{sign}(z)$ is the sign function, which returns +1 if $z \geq 0$ and -1 if $z < 0$.
\end{itemize}

The weights $w_i$ and the threshold are the parameters that define a specific perceptron hypothesis. The \textbf{hypothesis set} $H$ for the perceptron model consists of all possible functions $h(x)$ that can be formed by varying the weights $w$ and the threshold.  Each choice of weights and threshold gives a different hypothesis $h \in H$. The learning algorithm's goal is to find the optimal weights and threshold based on the training data, resulting in a final hypothesis $g$ that approximates the unknown target function $f$.

\subsection{Visualizing Linear Separability}
The perceptron model works best when the data is \textit{linearly separable}.  Consider a simplified 2-dimensional case where we want to classify customers as 'good' (+1) or 'bad' (-1) based on two features.  Linearly separable data means we can draw a straight line (in 2D) or a hyperplane (in higher dimensions) to perfectly separate the 'good' customers from the 'bad' customers.

\begin{figure}[H]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \begin{verbatim}
#save_to: linearly_separable_left.png
import matplotlib.pyplot as plt
import numpy as np

# Data points for linearly separable data (left plot)
X_left_pos = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])
X_left_neg = np.array([[1, 0], [2, 1], [3, 2], [4, 3]])

# Separating line for left plot
x_line_left = np.linspace(0, 5, 50)
y_line_left = 0.8 * x_line_left + 0.5

plt.figure(figsize=(6, 6))
plt.scatter(X_left_pos[:, 0], X_left_pos[:, 1], marker='+', s=100, c='blue', label='+1')
plt.scatter(X_left_neg[:, 0], X_left_neg[:, 1], marker='_', s=100, c='red', label='-1')
plt.plot(x_line_left, y_line_left, c='purple', label='Perceptron Boundary')
plt.fill_between(x_line_left, y_line_left, 6, color='lightblue', alpha=0.5)
plt.fill_between(x_line_left, y_line_left, -1, color='peachpuff', alpha=0.5)
plt.xlim(0, 6)
plt.ylim(-1, 6)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Linearly Separable Data (Left)')
plt.legend()
plt.grid(True)
plt.savefig('linearly_separable_left.png')
plt.close()
        \end{verbatim}
         \includegraphics[max width=\textwidth, max height=0.4\textheight, keepaspectratio]{linearly_separable_left.png}
        \caption{Linearly Separable Data - Initial Random Boundary}
        \label{fig:linearly_separable_left}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \begin{verbatim}
#save_to: linearly_separable_right.png
import matplotlib.pyplot as plt
import numpy as np

# Data points for linearly separable data (right plot) - same as left for visual consistency
X_right_pos = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])
X_right_neg = np.array([[1, 0], [2, 1], [3, 2], [4, 3]])


# Improved Separating line for right plot
x_line_right = np.linspace(0, 5, 50)
y_line_right = 1.2 * x_line_right - 0.2

plt.figure(figsize=(6, 6))
plt.scatter(X_right_pos[:, 0], X_right_pos[:, 1], marker='+', s=100, c='blue', label='+1')
plt.scatter(X_right_neg[:, 0], X_right_neg[:, 1], marker='_', s=100, c='red', label='-1')
plt.plot(x_line_right, y_line_right, c='purple', label='Perceptron Boundary')
plt.fill_between(x_line_right, y_line_right, 6, color='lightblue', alpha=0.5)
plt.fill_between(x_line_right, y_line_right, -1, color='peachpuff', alpha=0.5)


plt.xlim(0, 6)
plt.ylim(-1, 6)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Linearly Separable Data (Right)')
plt.legend()
plt.grid(True)
plt.savefig('linearly_separable_right.png')
plt.close()
        \end{verbatim}
        \includegraphics[max width=\textwidth, max height=0.4\textheight, keepaspectratio]{linearly_separable_right.png}
        \caption{Linearly Separable Data - Learned Boundary}
        \label{fig:linearly_separable_right}
    \end{minipage}
\end{figure}

In these figures, blue '+' symbols represent 'good' customers (+1), and red '-' symbols represent 'bad' customers (-1). The purple line represents a perceptron hypothesis, acting as a decision boundary. On one side of the line (light blue region), customers are classified as 'good', and on the other side (light peach region), as 'bad'. Initially, with random weights, the perceptron boundary (purple line in the left plot) might not correctly separate the data. The learning algorithm adjusts the weights to move this line, aiming to achieve perfect separation as shown in the right plot.

\subsection{Simplified Perceptron Hypothesis Notation}
To simplify the notation, we can incorporate the threshold into the weighted sum.  We introduce an artificial feature $x_0 = +1$ and a corresponding weight $w_0 = -\text{threshold}$.  The perceptron formula then becomes:

\[
h(x) = \text{sign}\left( \sum_{i=0}^{d} w_i x_i \right)
\]
where $x = (x_0, x_1, ..., x_d)$ and $w = (w_0, w_1, ..., w_d)$.  This can be further expressed in vector notation as:

\[
h(x) = \text{sign}(w^T x)
\]
where $w$ and $x$ are now column vectors, and $w^T$ denotes the transpose of $w$. This form represents the hypothesis as the sign of the inner product of the weight vector $w$ and the input vector $x$.

\subsection{Perceptron Learning Algorithm (PLA)}
The Perceptron Learning Algorithm (PLA) is an iterative algorithm used to find the weight vector $w$ that correctly classifies all training examples, assuming the data is linearly separable.

\textbf{Algorithm Steps:}
\begin{enumerate}
    \item \textbf{Initialization}: Start with an initial weight vector, e.g., $w = 0$.
    \item \textbf{Iteration}:
    \begin{enumerate}
        \item \textbf{Find a misclassified point}: Iterate through the training dataset $D = \{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\}$. Look for a point $(x_n, y_n)$ that is misclassified by the current weight vector $w$. A point is misclassified if $\text{sign}(w^T x_n) \neq y_n$.
        \item \textbf{Update weight vector}: If a misclassified point $(x_n, y_n)$ is found, update the weight vector using the rule:
        \[
        w \leftarrow w + y_n x_n
        \]
    \end{enumerate}
    \item \textbf{Repeat}: Repeat step 2 until no misclassified points are found in the dataset.
\end{enumerate}

The update rule intuitively adjusts the weight vector $w$ in the direction to correctly classify the misclassified point $x_n$.  If $y_n = +1$ and $w^T x_n < 0$ (misclassified as -1), adding $y_n x_n = x_n$ to $w$ tends to increase $w^T x_n$, making it more positive and thus more likely to be correctly classified as +1 in the next iteration. Similarly, if $y_n = -1$ and $w^T x_n > 0$ (misclassified as +1), adding $y_n x_n = -x_n$ tends to decrease $w^T x_n$, making it more negative and more likely to be correctly classified as -1.


\subsection{Iterations of PLA}

\begin{figure}[H]
    \centering
    \begin{verbatim}
#save_to: pla_iterations.png
import matplotlib.pyplot as plt
import numpy as np

# Data points - example linearly separable data
X_pla_pos = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])
X_pla_neg = np.array([[1, 0], [2, 1], [3, 2], [4, 3]])

# Misclassified point for illustration (example point from negative class misclassified above boundary)
misclassified_point = X_pla_neg[1]

# Current weight vector (example - leading to misclassification) - perpendicular to the purple line
w_initial = np.array([-1, 1]) # Example weight vector
origin = [0,0]

# Separating line based on initial w (example)
x_line_pla = np.linspace(0, 5, 50)
slope = -w_initial[0] / w_initial[1] if w_initial[1] != 0 else np.nan
intercept = 2 # Arbitrary intercept for visualization

if not np.isnan(slope):
    y_line_pla = slope * x_line_pla + intercept
else:
    y_line_pla = np.full_like(x_line_pla, np.nan) # Vertical line if w[1] is 0


plt.figure(figsize=(8, 8))
plt.scatter(X_pla_pos[:, 0], X_pla_pos[:, 1], marker='+', s=100, c='blue', label='+1')
plt.scatter(X_pla_neg[:, 0], X_pla_neg[:, 1], marker='_', s=100, c='red', label='-1')
plt.plot(x_line_pla, y_line_pla, c='purple', label='Initial Perceptron Boundary')
plt.scatter(misclassified_point[0], misclassified_point[1], facecolors='none', edgecolors='green', s=200, linewidth=2, label='Misclassified Point') # Highlight misclassified point


plt.arrow(misclassified_point[0], misclassified_point[1], 0.5 * w_initial[0], 0.5 * w_initial[1] ,
          head_width=0.1, head_length=0.2, fc='gray', ec='gray', label='Weight Update Direction') # Example arrow showing update direction

plt.fill_between(x_line_pla, y_line_pla, 8, color='lightblue', alpha=0.3)
plt.fill_between(x_line_pla, y_line_pla, -2, color='peachpuff', alpha=0.3)


plt.xlim(0, 6)
plt.ylim(-2, 8)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('PLA Iteration Example')
plt.legend()
plt.grid(True)
plt.savefig('pla_iterations.png')
plt.close()
        \end{verbatim}
    \includegraphics[max width=\textwidth, max height=0.4\textheight, keepaspectratio]{pla_iterations.png}
    \caption{One Iteration of PLA - Adjusting the Boundary}
    \label{fig:pla_iterations}
\end{figure}
In each iteration of PLA, the algorithm picks a misclassified point and adjusts the perceptron boundary (purple line) to correctly classify this point. As illustrated, if a blue '+' point falls into the peach puff region (incorrectly classified), or a red '-' point falls into the light blue region (incorrectly classified), the PLA update rule moves the purple line in a direction that reduces this misclassification.  This iterative adjustment, when applied repeatedly to misclassified points, eventually leads to a boundary that correctly separates all points, provided the data is linearly separable.

\subsection{Convergence of PLA}
A crucial property of the PLA is its convergence guarantee. If the training data is indeed linearly separable, the PLA is guaranteed to converge to a weight vector $w$ that correctly classifies all training examples in a finite number of iterations. This means the algorithm will eventually find a perceptron hypothesis that perfectly separates the data. However, if the data is not linearly separable, the PLA will not converge, and may keep adjusting the weights indefinitely, or oscillate.

\section{Types of Learning}
\subsection{Premise of Learning}
At its core, learning, in a broad sense, is about using a set of observations to uncover an underlying process.  This premise is not unique to machine learning; it is shared across various disciplines, including statistics. In statistics, for example, the underlying process is often a probability distribution, and observations are samples drawn from that distribution. The goal is to infer the distribution from the samples.

\subsection{Categorizing Learning Types}
Different types of learning emerge when this fundamental premise is applied in different contexts, often necessitating distinct mathematical tools and algorithms.  Here are three major types of learning:

\begin{enumerate}
    \item \textbf{Supervised Learning}
    \item \textbf{Unsupervised Learning}
    \item \textbf{Reinforcement Learning}
\end{enumerate}

\subsection{Supervised Learning}
In \textbf{supervised learning}, the training data is "supervised" because for each input example, the correct output or target value is explicitly given.  The algorithm learns to map inputs to outputs based on these labeled examples.

\subsubsection{Example: Supervised Coin Recognition}
Consider a vending machine that needs to recognize different types of coins.  We can measure physical attributes of coins, such as size and mass. In supervised learning, we would have a dataset where each coin is described by its size and mass, and is also labeled with its denomination (e.g., 25 cents, 10 cents, 5 cents, 1 cent).

\begin{figure}[H]
    \centering
    \begin{verbatim}
#save_to: supervised_coin_recognition.png
import matplotlib.pyplot as plt
import numpy as np

# Sample data for coin recognition (Supervised)
# Using numpy arrays for cleaner handling
coin_data_supervised = {
    '25': np.random.multivariate_normal([7, 7], [[0.3, 0], [0, 0.3]], 50), # Quarters - Green
    '5': np.random.multivariate_normal([5, 5], [[0.2, 0], [0, 0.2]], 50),  # Nickels - Red
    '1': np.random.multivariate_normal([3, 3], [[0.15, 0], [0, 0.15]], 50), # Pennies - Yellow
    '10': np.random.multivariate_normal([2, 6], [[0.1, 0], [0, 0.1]], 50) # Dimes - Blue
}

colors = {'25': 'limegreen', '5': 'red', '1': 'gold', '10': 'cornflowerblue'}

plt.figure(figsize=(8, 6))

for coin_value, data in coin_data_supervised.items():
    plt.scatter(data[:, 0], data[:, 1], c=colors[coin_value], label=coin_value, s=20)
    plt.annotate(coin_value, xy=(np.mean(data[:,0]), np.mean(data[:,1])), textcoords='offset points', xytext=(5,5), ha='center')


plt.xlabel('Size')
plt.ylabel('Mass')
plt.title('Supervised Coin Recognition')
plt.legend(title='Cents')
plt.grid(True)
plt.xlim(0, 10)
plt.ylim(0, 10)
plt.savefig('supervised_coin_recognition.png')
plt.close()


    \end{verbatim}
    \includegraphics[max width=\textwidth, max height=0.4\textheight, keepaspectratio]{supervised_coin_recognition.png}
    \caption{Supervised Coin Recognition Data}
    \label{fig:supervised_coin_recognition}
\end{figure}

In the figure above, each cluster of points represents a type of coin, color-coded and labeled with its value.  The supervised learning algorithm uses this labeled data to learn boundaries that can classify future, unlabeled coins into their correct denominations.  Once trained, the system can recognize new coins based on their size and mass measurements by determining which region they fall into, delimited by learned boundaries.


\subsection{Unsupervised Learning}
In \textbf{unsupervised learning}, the training data is unlabeled.  The algorithm only receives input data without corresponding correct outputs.  The goal is to find patterns, structures, or groupings within the data itself, without explicit guidance.

\subsubsection{Example: Unsupervised Coin Recognition}
Imagine the same coin recognition problem, but now the vending machine only measures the size and mass of coins without knowing their denominations. The data is just a collection of measurements, without labels indicating coin values.

\begin{figure}[H]
    \centering
    \begin{verbatim}
#save_to: unsupervised_coin_recognition.png
import matplotlib.pyplot as plt
import numpy as np

# Sample data for coin recognition (Unsupervised) - same data, but unlabeled
coin_data_unsupervised = {
    'Type 1': np.random.multivariate_normal([7, 7], [[0.3, 0], [0, 0.3]], 50), # Quarters - Green
    'Type 2': np.random.multivariate_normal([5, 5], [[0.2, 0], [0, 0.2]], 50),  # Nickels - Red
    'Type 3': np.random.multivariate_normal([3, 3], [[0.15, 0], [0, 0.15]], 50), # Pennies - Yellow
    'Type 4': np.random.multivariate_normal([2, 6], [[0.1, 0], [0, 0.1]], 50) # Dimes - Blue
}

colors = {'Type 1': 'gray', 'Type 2': 'gray', 'Type 3': 'gray', 'Type 4': 'gray'} # All gray - unlabeled

plt.figure(figsize=(8, 6))

for coin_type, data in coin_data_unsupervised.items():
    plt.scatter(data[:, 0], data[:, 1], c=colors[coin_type], label=coin_type, s=20)


plt.xlabel('Size')
plt.ylabel('Mass')
plt.title('Unsupervised Coin Recognition')
plt.legend(title='Coin Types')
plt.grid(True)
plt.xlim(0, 10)
plt.ylim(0, 10)
plt.savefig('unsupervised_coin_recognition.png')
plt.close()

    \end{verbatim}
    \includegraphics[max width=\textwidth, max height=0.4\textheight, keepaspectratio]{unsupervised_coin_recognition.png}
    \caption{Unsupervised Coin Recognition Data}
    \label{fig:unsupervised_coin_recognition}
\end{figure}

As shown in the figure, without labels, the data points still cluster into distinct groups. An unsupervised learning algorithm can identify these clusters, effectively categorizing the coins into different types based on their physical attributes.  Although the algorithm doesn't know that "Type 1" is a quarter or "Type 4" is a dime, it successfully groups similar coins together.  This form of learning is useful for exploratory data analysis, clustering, dimensionality reduction, and discovering hidden structures in data.

\subsection{Reinforcement Learning}
In \textbf{reinforcement learning}, the algorithm interacts with an environment to learn optimal behavior.  It receives feedback in the form of rewards or penalties based on its actions. There are no explicit correct outputs provided for inputs; instead, the learning is driven by maximizing cumulative rewards.

\subsubsection{Example: Toddler Learning about Hot Tea}
Consider a toddler learning not to touch a hot cup of tea. The input is the sight of a tea cup. Initially curious, the toddler might reach out to touch it. The action of touching results in a negative reward (pain, "Ouch!"). This negative experience reinforces the association between touching a steaming cup and pain. Over time, through repeated experiences and feedback (rewards or penalties), the toddler learns to avoid touching hot cups. The learning is not from explicit instructions but from the consequences of actions in an environment. Reinforcement learning is about learning to make sequences of decisions to achieve a long-term goal, guided by feedback from the environment.

\subsubsection{Application: Game Playing}
Reinforcement learning is prominently used in training AI agents to play games. For example, a computer learning to play backgammon might make a move (action) based on the current game state (input). The outcome of the game (win or loss) serves as a reward signal. The algorithm learns to make moves that maximize its chances of winning over many games, without being explicitly told the optimal move in each situation.


\section{A Learning Puzzle: The Limits of Learning}
\subsection{The Puzzle}
Consider the following learning puzzle. We are given training examples in the form of 3x3 grids of black and white squares, each labeled with either -1 or +1. The task is to learn a function that predicts the label for a new, unlabeled 3x3 grid.

\begin{figure}[H]
    \centering
    \begin{verbatim}
#save_to: learning_puzzle.png
import matplotlib.pyplot as plt
import matplotlib.patches as patches

patterns_neg = [
    [[1, 0, 0], [1, 1, 0], [0, 1, 0]], # Pattern 1 for -1
    [[0, 1, 0], [1, 1, 1], [0, 1, 0]], # Pattern 2 for -1
    [[0, 0, 1], [0, 1, 1], [0, 0, 1]]  # Pattern 3 for -1
]

patterns_pos = [
    [[0, 0, 0], [1, 1, 1], [1, 0, 1]], # Pattern 1 for +1
    [[0, 1, 0], [1, 1, 1], [0, 1, 0]], # Pattern 2 for +1
    [[1, 1, 1], [0, 1, 0], [0, 1, 0]]  # Pattern 3 for +1
]

test_pattern = [[0, 1, 0], [1, 1, 0], [0, 1, 0]] # Test pattern

def display_pattern(ax, pattern, label=None):
    for i in range(3):
        for j in range(3):
            if pattern[i][j] == 1:
                rect = patches.Rectangle((j, 2-i), 1, 1, facecolor='black')
                ax.add_patch(rect)
            else:
                rect = patches.Rectangle((j, 2-i), 1, 1, facecolor='white', edgecolor='black', linewidth=0.5)
                ax.add_patch(rect)
    ax.set_xlim([0, 3])
    ax.set_ylim([0, 3])
    ax.set_xticks([])
    ax.set_yticks([])
    if label:
        ax.set_title(label)

fig, axes = plt.subplots(3, 3, figsize=(6, 6))

for i in range(3):
    display_pattern(axes[0, i], patterns_neg[i])
    if i == 2:
        axes[0, i].set_ylabel('f = -1', rotation=0, labelpad=30, ha='right') # Label only once

for i in range(3):
    display_pattern(axes[1, i], patterns_pos[i])
    if i == 2:
        axes[1, i].set_ylabel('f = +1', rotation=0, labelpad=30, ha='right') # Label only once


display_pattern(axes[2, 1], test_pattern, label='f = ?') # Test pattern in the middle of the last row
for i in [0, 2]: # Hide unused axes in the last row
    axes[2, i].axis('off')


plt.tight_layout()
plt.savefig('learning_puzzle.png')
plt.close()
    \end{verbatim}
    \includegraphics[max width=\textwidth, max height=0.4\textheight, keepaspectratio]{learning_puzzle.png}
    \caption{Learning Puzzle Patterns}
    \label{fig:learning_puzzle}
\end{figure}

The top row of grids are labeled with $f=-1$, and the middle row are labeled with $f=+1$. The bottom grid is the test point for which we want to predict the label, $f = ?$.

\subsection{The Ambiguity of Learning}
Upon examining the puzzle, one might notice different potential patterns. For example, one could hypothesize that the function is +1 if the pattern is symmetric, or -1 if the top-left square is black.  However, different individuals might perceive different patterns, leading to varying predictions for the test point (+1 or -1).

This divergence in answers highlights a fundamental issue: with a finite training set, there can be multiple functions that are consistent with the given data but differ on unseen points. Since the \textit{true} target function is unknown, and we only have a limited set of examples, we cannot definitively determine the correct label for the test point based solely on the provided data.  The function could indeed be anything.

\subsection{Implications for Learning}
The learning puzzle illustrates a critical concept: \textbf{learning from data is inherently ambiguous}. If the target function is truly unknown, and we only observe its values at a finite number of points, we cannot definitively extrapolate its behavior to unobserved points.  There are infinitely many functions that could explain the given training data.

Does this mean learning is impossible?  Not necessarily.  While we cannot be certain about predictions on unseen data, machine learning is still possible and incredibly useful. The key is to understand under what conditions and to what extent we can reliably generalize from training data to unseen data. The next lecture will delve into the question of when and why learning is feasible, despite the inherent ambiguity illustrated by this puzzle.

\section{Conclusion}
This chapter introduced the fundamental problem of learning and the key components involved. We explored the essence of machine learning through the movie rating example, formalized the learning problem using the credit approval metaphor, and introduced the perceptron model as a simple learning algorithm.  We also categorized different types of learning and concluded with a puzzle demonstrating the inherent ambiguity and challenges in learning from limited data.  The critical questions raised about the feasibility and limits of learning will be addressed in the subsequent lectures, starting with the next lecture focusing on whether learning is indeed feasible.

\end{document}
```